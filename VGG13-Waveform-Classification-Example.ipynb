{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7a16f1-18b7-4fe7-a7ff-536cdab17b6e",
   "metadata": {},
   "source": [
    "## PyTorch VGGNet13 example for RadarML Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f2252-a489-4eb7-8925-18c83a293898",
   "metadata": {},
   "source": [
    "#### Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6846a376-256e-471b-a61d-3cab3296c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.io as io\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215d572c-4ec8-4326-8456-66bbdad38d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1173d-f48e-4df6-85f3-31369a5ca11c",
   "metadata": {},
   "source": [
    "#### Detect if model will run on CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6413c609-bede-43d3-a551-df3fbdeaac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ded25-0bc4-446a-a1be-4598ae1812e3",
   "metadata": {},
   "source": [
    "#### Spectogram Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb418caf-9ed9-4140-ba61-6b830a148f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "fs = 3.84e9/32    # Final sample rate given by RF sample rate divided by the total decimation\n",
    "\n",
    "def normalise_to_uint8(value,db=0,dynamic_range=40):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize a float64 array or a single float64 value to uint8.\n",
    "    If input is a single value, it will return the normalized value as uint8.\n",
    "    If input is an array, it returns a normalized uint8 array.\n",
    "    \"\"\"\n",
    "    if isinstance(value, np.ndarray):\n",
    "        # If it's an array, normalize the entire array\n",
    "\n",
    "        if db:\n",
    "            #print(\"Dynamic Range\")\n",
    "            max_val = np.max(value)\n",
    "            min_val = np.max([int(max_val-dynamic_range),int(np.min(value))],)\n",
    "            value[value<min_val]=min_val\n",
    "        else:\n",
    "            #print(\"Using frame normalisation with no dynamic range limits\")\n",
    "            min_val = np.min(value)\n",
    "            max_val = np.max(value)\n",
    "        \n",
    "        # Normalize and clip to the range [0, 255]\n",
    "        \n",
    "        \n",
    "        normalized = np.clip((value - min_val) / (max_val - min_val) * 255, 0, 255)\n",
    "        \n",
    "        # Convert to uint8\n",
    "        return normalized.astype(np.uint8)\n",
    "    else:\n",
    "        # If it's a single float64 value, normalize it (with respect to its own range)\n",
    "        return np.uint8(np.clip(value * 255, 0, 255))  # Normalize to the range [0, 255] and convert to uint8\n",
    "\n",
    "\n",
    "def SpectrogramGenerator(cmplx_data, filename):\n",
    "\n",
    "    NFFT = 256\n",
    "    noverlap = 128\n",
    "    L = cmplx_data.shape[0]\n",
    "    step = NFFT - noverlap       # hop size (128 for 256 FFT w/ 50% overlap)\n",
    "    if L < NFFT:\n",
    "        num_segments = 0\n",
    "    else:\n",
    "        num_segments = 1 + (L - NFFT) // step\n",
    "\n",
    "    freq_data = np.zeros((num_segments, NFFT), dtype=complex)\n",
    "    window = np.hanning(NFFT)\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * step\n",
    "        segment = cmplx_data[start : start + NFFT] * window\n",
    "        freq_data[i, :] = np.fft.fft(segment, NFFT)\n",
    "\n",
    "    freq_data = np.fft.fftshift(freq_data, axes=1)\n",
    "    freq_data = freq_data.T\n",
    "    \n",
    "    NQ = fs / 2\n",
    "    ylength = freq_data.shape[0]\n",
    "\n",
    "    power = np.abs(freq_data) ** 2\n",
    "    power_db = 10 * np.log10(power + 1e-12) # Add small ammount to avoid log 0 \n",
    "\n",
    "    img=np.zeros((freq_data.shape[0], freq_data.shape[1], 1))\n",
    "    img[:,:,0] = normalise_to_uint8(power_db)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.imwrite(filename, img)\n",
    "    \n",
    "    del img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ffe09-3685-4537-898d-0208d23195a0",
   "metadata": {},
   "source": [
    "## Generate Spectrograms from processed (SNR degraded) numpy files\n",
    "\n",
    "#### Make sure the \"target_SNR_dBS\" list matches that used in the \"make_dataset.py\" script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d001e5-9835-462c-8866-f9c452ab0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# This is used to map the index in the dataset to an SNR value (This needs to match the \"make_dset.py\" script)\n",
    "target_SNR_dBs = [30, 27, 24, 21, 18, 15, 12, 9, 6, 3, 0, -3, -6, -9, -12, -15, -18, -21, -24, -27, -30]\n",
    "\n",
    "# Define what SNRs to use in training and testing\n",
    "train_SNR_dBs = [snr for snr in target_SNR_dBs if snr >= 0]    # Use SNRs above and including 0dB\n",
    "test_SNR_dBs  = target_SNR_dBs                                 # Use all SNRs for testing\n",
    "\n",
    "# Define what train/test split to use\n",
    "train_test_split = 0.8\n",
    "\n",
    "# Core function to process a single .npy file\n",
    "def process_file(data):\n",
    "\n",
    "    file_path = data[0]\n",
    "    total_pulses = data[1]\n",
    "\n",
    "    fname = os.path.basename(file_path)\n",
    "    name_without_ext = os.path.splitext(fname)[0]\n",
    "\n",
    "    modulation_part = name_without_ext.split('_')[1]\n",
    "    modulation_name = modulation_part.split('#')[0]\n",
    "    \n",
    "    raw_data = np.load(file_path)\n",
    "\n",
    "    #for channel_idx in [0]:\n",
    "    for channel_idx in range(len(raw_data)):\n",
    "        # Add modulation_name as a subfolder here:\n",
    "        channel_train_folder = os.path.join(root_spectrogram_folder, f\"ADC{channel_idx}\", \"train\", modulation_name)\n",
    "        channel_test_folder  = os.path.join(root_spectrogram_folder, f\"ADC{channel_idx}\", \"test\", modulation_name)\n",
    "\n",
    "        os.makedirs(channel_train_folder, exist_ok=True)\n",
    "        os.makedirs(channel_test_folder, exist_ok=True)\n",
    "\n",
    "        max_available_pulses = raw_data.shape[2]  # Assumes shape = [channels][snrs][pulses]\n",
    "        pulses_to_use = min(total_pulses, max_available_pulses)\n",
    "\n",
    "        #print(fname)\n",
    "        #print(max_available_pulses)\n",
    "        #print(pulses_to_use)\n",
    "        \n",
    "        split_index = int(pulses_to_use * train_test_split)\n",
    "\n",
    "        for pulse_idx in range(pulses_to_use):\n",
    "            is_training_pulse = pulse_idx < split_index\n",
    "\n",
    "            for snr_idx, snr_db in enumerate(target_SNR_dBs):\n",
    "                if is_training_pulse:\n",
    "                    if snr_db not in train_SNR_dBs:\n",
    "                        continue\n",
    "                    current_dir = channel_train_folder\n",
    "                else:\n",
    "                    if snr_db not in test_SNR_dBs:\n",
    "                        continue\n",
    "                    current_dir = channel_test_folder\n",
    "\n",
    "                row_data = raw_data[channel_idx][snr_idx][pulse_idx]\n",
    "                cmplx_data = row_data[0::2] + 1.0j * row_data[1::2]\n",
    "                filename = f\"{name_without_ext}_{channel_idx}_{snr_db}dB_{pulse_idx}.png\"\n",
    "                filepath = os.path.join(current_dir, filename)\n",
    "                SpectrogramGenerator(cmplx_data, filepath)\n",
    "\n",
    "# Define root folder to store our Spectrogram Datasets\n",
    "root_spectrogram_folder = os.path.join(os.getcwd(), \"SpectrogramData\")\n",
    "os.makedirs(root_spectrogram_folder, exist_ok=True)\n",
    "\n",
    "root_npy_folder = os.path.join(os.getcwd(), \"processed\")\n",
    "\n",
    "# Count waveform occurences\n",
    "waveform_counts = defaultdict(int)\n",
    "type_to_files = defaultdict(list)\n",
    "\n",
    "for file in os.listdir(root_npy_folder):\n",
    "    if not file.endswith(\".npy\"):\n",
    "        continue\n",
    "\n",
    "    # Get waveform type (e.g. 'barker5x13', 'fsk', 'nlfm')\n",
    "    waveform_type = file.split('_')[1].lower()\n",
    "    waveform_type = waveform_type.split('#')[0]  # Remove trailing # index\n",
    "\n",
    "    # Count and group\n",
    "    waveform_counts[waveform_type] += 1\n",
    "    type_to_files[waveform_type].append(file)\n",
    "\n",
    "# Print number of waveform types\n",
    "for waveform, count in waveform_counts.items():\n",
    "    print(f\"{waveform} = {count}\")\n",
    "\n",
    "# Determine balancing values\n",
    "max_count = max(waveform_counts.values())\n",
    "chosen_count = max_count * 5  # You can tweak this multiplier\n",
    "#print(f\"\\nMax count = {max_count}\")\n",
    "#print(f\"Chosen per-type total = {chosen_count}\")\n",
    "file_repeat_pairs = []\n",
    "\n",
    "for waveform_type, files in type_to_files.items():\n",
    "    n_files = len(files)\n",
    "    repeats_per_file = int(np.ceil(chosen_count / n_files))\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root_npy_folder, file)\n",
    "        file_repeat_pairs.append((file_path, repeats_per_file))\n",
    "\n",
    "# Print number of repeats per waveform to achieve balanced set\n",
    "print(\"\\nSample of file_repeat_pairs:\")\n",
    "for pair in file_repeat_pairs:\n",
    "    print(f\"{os.path.basename(pair[0])}, {pair[1]}\")\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "max_workers = max(1, int(num_cpus * 0.8))\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    list(tqdm(executor.map(process_file, file_repeat_pairs), total=len(file_repeat_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fd4d6-44d8-443c-bf58-404c7014f83c",
   "metadata": {},
   "source": [
    "#### PyTorch Dataloader code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1241b5cb-47c7-4614-b5bc-e2b78423bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import io\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_to_idx=None):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        # If no mapping is provided, create it\n",
    "        if class_to_idx is None:\n",
    "            # Map class names to integer labels\n",
    "            classes = sorted(entry.name for entry in os.scandir(root_dir) if entry.is_dir())\n",
    "            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "\n",
    "        # This 2D array stores [image_path, label] pairs\n",
    "        self.data = []\n",
    "        for cls_name, label in self.class_to_idx.items():\n",
    "            cls_folder = os.path.join(root_dir, cls_name)\n",
    "            if not os.path.isdir(cls_folder):\n",
    "                continue\n",
    "            for fname in os.listdir(cls_folder):\n",
    "                if fname.lower().endswith('.png'):\n",
    "                    full_path = os.path.join(cls_folder, fname)\n",
    "                    self.data.append([full_path, label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "\n",
    "        # Load image and normalize\n",
    "        image_tensor = io.read_image(img_path).float() / 255.0\n",
    "\n",
    "        return image_tensor, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0267bb-49e0-49d9-ba11-506a219b5752",
   "metadata": {},
   "source": [
    "#### Definition of VGG based network single channel network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c181f6-c3fb-429e-9990-936bb55a1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG13_net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # The input data is 1x224x224\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 24, (3, 3), 1, 1),            # Output = 24*224*224\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(24, 24, (3, 3), 1, 1),           # Output = 24*224*224\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),                        # Output = 24*112*112\n",
    "\n",
    "            nn.Conv2d(24, 48, (3, 3), 1, 1),           # Output = 48*112*112\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(48, 48, (3, 3), 1, 1),           # Output = 48*112*112\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),                        # Output = 48*56*56\n",
    "\n",
    "            nn.Conv2d(48, 96, (3, 3), 1, 1),           # Output = 96*56*56\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(96, 96, (3, 3), 1, 1),           # Output = 96*56*56\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),                        # Output = 96*28*28\n",
    "\n",
    "            nn.Conv2d(96, 192, (3, 3), 1, 1),          # Output = 192*28*28\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, (3, 3), 1, 1),         # Output = 192*28*28\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),                        # Output = 192*14*14\n",
    "\n",
    "            nn.Conv2d(192, 192, (3, 3), 1, 1),         # Output = 192*14*14\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, (3, 3), 1, 1),         # Output = 192*14*14\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),                        # Output = 192*7*7\n",
    "        )\n",
    "\n",
    "        # Final feature map = 192 * 7 * 7 = 9408\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(192 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33182d49-2704-432a-86bb-7d8c11fc4793",
   "metadata": {},
   "source": [
    "## Training and Validation Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9456f9e2-5607-4022-9818-00faeaa3e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "adc_num = 0\n",
    "vgg_batchSize = 32\n",
    "\n",
    "train_dataset = SpectrogramDataset(root_dir=f'SpectrogramData/ADC{adc_num}/train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=vgg_batchSize, shuffle=True)\n",
    "\n",
    "test_dataset = SpectrogramDataset(root_dir=f'SpectrogramData/ADC{adc_num}/test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=vgg_batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e60646-3f4c-489f-80ee-92853b94ad42",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46657c9-6eb1-4d24-b1fc-2afbf0edad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 40\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     41\u001b[0m     train_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     43\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup some tunable parameters for training\n",
    "vgg_epochs = 100\n",
    "vgg_momentum = 0.9\n",
    "vgg_learningRate = 1e-4\n",
    "vgg_weightDecay = 5e-4\n",
    "\n",
    "num_unique_waveforms = len(train_dataset.class_to_idx)\n",
    "\n",
    "model = VGG13_net(num_classes=num_unique_waveforms)\n",
    "model.to(device)\n",
    "\n",
    "# Setup loss function and optimiser\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=vgg_learningRate)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "patience = 10 \n",
    "epochs_no_improve = 0\n",
    "best_model_path = f\"weights/adc{adc_num}_best_model.pth\"\n",
    "last_model_path = f\"weights/adc{adc_num}_last_model.pth\"\n",
    "\n",
    "# Train model\n",
    "for epoch in range(vgg_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{vgg_epochs} - Training\", leave=False)\n",
    "    \n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"\\n[Epoch {epoch+1}] Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    test_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{vgg_epochs} - Testing\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_bar.set_postfix(accuracy=100 * correct / total)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"[Epoch {epoch+1}] Test Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    if accuracy >= best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"[Epoch {epoch+1}] ✅ Best model saved with accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Always save last model so that training can be resumed\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    # Check early stopping condition\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"⏹️ Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "        break  # exits the epoch loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a1a1d-1e01-4f7d-99d1-bf688b69541c",
   "metadata": {},
   "source": [
    "## Validation (Confusion Matrix Per SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e20c57-60d8-47f7-b322-8ac308652a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SNR -30dB: 2304 samples\n",
      "\n",
      "Processing SNR -27dB: 2304 samples\n",
      "\n",
      "Processing SNR -24dB: 2304 samples\n",
      "\n",
      "Processing SNR -21dB: 2304 samples\n",
      "\n",
      "Processing SNR -18dB: 2304 samples\n",
      "\n",
      "Processing SNR -15dB: 2304 samples\n",
      "\n",
      "Processing SNR -12dB: 2304 samples\n",
      "\n",
      "Processing SNR -9dB: 2304 samples\n",
      "\n",
      "Processing SNR -6dB: 2304 samples\n",
      "\n",
      "Processing SNR -3dB: 2304 samples\n",
      "\n",
      "Processing SNR 0dB: 2304 samples\n",
      "\n",
      "Processing SNR 3dB: 2304 samples\n",
      "\n",
      "Processing SNR 6dB: 2304 samples\n",
      "\n",
      "Processing SNR 9dB: 2304 samples\n",
      "\n",
      "Processing SNR 12dB: 2304 samples\n",
      "\n",
      "Processing SNR 15dB: 2304 samples\n",
      "\n",
      "Processing SNR 18dB: 2304 samples\n",
      "\n",
      "Processing SNR 21dB: 2304 samples\n",
      "\n",
      "Processing SNR 24dB: 2304 samples\n",
      "\n",
      "Processing SNR 27dB: 2304 samples\n",
      "\n",
      "Processing SNR 30dB: 2304 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Organise test data by modulation type and SNR so we can form Conf matrix\n",
    "def organise_data_by_modulation_and_snr(root_dir):\n",
    "    data_structure = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for modulation_folder in os.listdir(root_dir):\n",
    "        modulation_path = os.path.join(root_dir, modulation_folder)\n",
    "        \n",
    "        if not os.path.isdir(modulation_path):\n",
    "            continue\n",
    "        \n",
    "        for filename in os.listdir(modulation_path):\n",
    "            if not filename.lower().endswith('.png'):\n",
    "                continue\n",
    "            \n",
    "            match = re.search(r'(-?\\d+)dB', filename, re.IGNORECASE)\n",
    "            if match:\n",
    "                snr = int(match.group(1))\n",
    "                full_path = os.path.join(modulation_path, filename)\n",
    "                data_structure[modulation_folder][snr].append(full_path)\n",
    "    \n",
    "    return data_structure\n",
    "\n",
    "\n",
    "# Dataset that only contains desired sub-sets\n",
    "class FilteredDataset(Dataset):\n",
    "    def __init__(self, file_paths, class_to_idx):\n",
    "        self.file_paths = file_paths\n",
    "        self.class_to_idx = class_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        modulation = os.path.basename(os.path.dirname(img_path))\n",
    "        label = self.class_to_idx[modulation]\n",
    "        image_tensor = io.read_image(img_path).float() / 255.0\n",
    "        return image_tensor, label\n",
    "\n",
    "# setup dataset\n",
    "root_dir = f'SpectrogramData/ADC{adc_num}/test'\n",
    "data_structure = organise_data_by_modulation_and_snr(root_dir)\n",
    "\n",
    "# Create classes/index mappings\n",
    "classes = sorted(entry.name for entry in os.scandir(root_dir) if entry.is_dir())\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "idx_to_class = {idx: cls_name for cls_name, idx in class_to_idx.items()}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_unique_waveforms = len(test_dataset.class_to_idx)\n",
    "\n",
    "# Load our model and pre-trained weights\n",
    "model = VGG13_net(num_unique_waveforms).to(device)\n",
    "model.load_state_dict(torch.load(f\"weights/adc{adc_num}_best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Group files by SNR across all modulations\n",
    "snr_to_files = defaultdict(list)\n",
    "for modulation in data_structure:\n",
    "    for snr in data_structure[modulation]:\n",
    "        snr_to_files[snr].extend(data_structure[modulation][snr])\n",
    "\n",
    "# Process each SNR level\n",
    "for snr in sorted(snr_to_files.keys()):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Create dataset subset for this SNR\n",
    "    filtered_dataset = FilteredDataset(snr_to_files[snr], class_to_idx)\n",
    "    dataloader = DataLoader(filtered_dataset, batch_size=vgg_batchSize, shuffle=False)\n",
    "    \n",
    "    # Collect all predictions and labels\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "\n",
    "           images = images.to(device)\n",
    "           outputs = model(images)\n",
    "           predictions = outputs.argmax(dim=1)\n",
    "            \n",
    "           all_labels.extend(labels.cpu().numpy())\n",
    "           all_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    # Convert to indexes back to class names\n",
    "    true_labels = [idx_to_class[idx] for idx in all_labels]\n",
    "    pred_labels = [idx_to_class[idx] for idx in all_predictions]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        true_labels, \n",
    "        pred_labels, \n",
    "        labels=classes,\n",
    "        normalize='true'\n",
    "    )\n",
    "\n",
    "    cm_display = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, \n",
    "        display_labels=classes\n",
    "    )\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax, cmap=\"viridis\", values_format=\".2f\", colorbar=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    plt.title(f'Confusion Matrix at SNR = {snr}dB for ADC{adc_num}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'adc{adc_num}_confusion_matrix_snr_{snr}dB.png', dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0fb31-5df5-4b1d-a62b-5ff67698561c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiChannelRESM",
   "language": "python",
   "name": "multichannelresm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
